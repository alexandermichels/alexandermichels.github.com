<!-- Blog Posts -->
<article>
  <header>
    <a href="https://alexandermichels.github.io/blog/OpenInformationExtraction.html"><h2>Open Information Extraction</h2></a>
    <address>
      <a href="https://alexandermichels.github.io/about_me.html">Alexander Michels</a>
    </address>
  </header>

  <center><h2 id="introduction">Introduction</h2></center>

  <p>Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Traditionally, this has required extensive work crafting extraction rules or hand-tagging training examples and each relation of interest had to be specified either with rules or examples. The time investment also meant that IE was generally performed on small, structured, and homogeneous corpora allowing systems to use linguistic technologies that were not computational taxing and not very robust because they didn't have to be such as dependency parsers and Named-Entity Recognizers. As the size of the relations and the size of the corpus scale, all of these techniques become infeasible.</p>

  <div align="center" >
    <img alt="Information Extraction graphic" src="../images/InformationExtraction.jpg"  style="max-width:100%;">
  </div>

  <p>This problem was solved with the revolutionary introduction of Open Information Extraction by Banko, Cafarella, Soderland, Broadhead, and Etzioni, in their <a href="https://github.com/alexandermichels/AIReading/blob/master/pdf/Open_Information_Extraction_from_the_Web.pdf">2007 "Open Information Extraction from the Web"</a>. Open Information Extraction (IE) is the task of extracting assertions from large sets of documents (known as a corpora) without requiring a pre-specified vocabulary. Their TextRunner used a self-supervised learner to identify relational dependencies between nouns and form subject-predicate-object triples. How does this work? Their learner automatically labels its own training data by parser subject-predicate-object triples and labeling them as good or bad based on a few constraints such as relationships in the parse tree. These triples are then mapped to feature vector representations and a Naive Bayes classifier is trained on this data which is used by the extractor.</p>

  <div align="center" >
    <img alt="A parse tree" src="../images/ParseTree.png"  style="max-width:100%;">
  </div>

    <p style="visibility:hidden">break</p>

  <center><h2 id="challengesinopeninformationextraction">Challenges in Open Information Extraction</h2></center>

  <p>We are working with impressive systems. Even the first OpenIE system, TextRunner used: a parser, sets of non-specific linguistic rules using Part-of-Speech tagging and Parse trees, self labeling of training data for a Naive Bayes classifier which is used by an extractor, and then a redundancy-based assessor module. However, language is extremely complex and Open (Unsupervised) Information Extraction is an incredibly difficult task with a set of hard to tackle problems.</p>

  <h4 id="entityresolution">Entity Resolution</h4>

  <p>Ironically, this is a problem with many names: Entity Resolution, Record Linkage, Object Identification, Instance Matching, and Deduplication, but it refers to the problem of knowing when two different labels are referring to same thing. For example, "Barack Obama" and "Obama", but not all are so easy. Sometimes you have entities named with abbreviations or other missing information, and humans are able to infer it from the context, but when the only information we have is a subject-predicate-object triple, there isn't much context. A closely related problem is relation resolution, knowing when two relations have the same semantic meaning. For example, ("X", "was founded by", "Y") and ("X", "was originally founded by", "Y") mean the same thing, but some small changes like that can drastically alter the meaning of a relational phrase, so it can be difficult to know when to merge two tuples.</p>

  <h4 id="incoherentextractions">Incoherent Extractions</h4>

  <p>Incoherent Extractions are relational phrases which have no meaningful interpretations. They occur because the extractor decides whether to include each word in a relational phrase on a case by case basis which often results in incomprehensible phrases such as "was central torpedo".  Incoherent extractions account for 1-7% of Open IE system outputs. Approaches to solving this problem revolve around syntactic constraints on removal and inclusion of words. <a href="https://github.com/alexandermichels/AIReading/blob/master/pdf/Identifying_Relations_for_Open_Information_Extraction.pdf">Fader, Soderland, and Etzioni</a> propose that every multi-word relation phrase must begin with a verb, end with a preposition, and be contiguous sequence of words in the sentence.</p>

  <h4 id="uninformativeextractions">Uninformative Extractions</h4>

  <p>Uninformative extractions are those that omit critical information from the text resulting in a tuple that is no longer informative. Pronouns can lead to results such as ("He", "likes", "it") which doesn't have an real meaning. Consider the more subtle example <em>"Faust made a deal with the devil"</em>. This approach would yield ("Faust", "made", "a deal") or ("Faust", "made", "the devil") both of which are wrong because this sentence contains a light verb construction which is a multi-word expressions composed a verb and a noun where the noun carries the semantic content of the predicate (i.e. "making the deal"). English has a variety of problems complicating this process. Uninformative extractions account for below 4-7% of Open IE system outputs.</p>

  <div align="center">
    <img alt="Overview of the technologies in Information Extraction" src="../images/TextandDataMiningTechniques.jpg"  style="max-width:100%;">
  </div>

    <p style="visibility:hidden">break</p>

  <center><h2 id="conclusion">Conclusion</h2></center>

  <p>So if you've been wondering how we are tacking all of the problems I talked about in my <a href="https://alexandermichels.github.io/blog/WebCrawling101.html">Web Crawling</a> post, here is a partial answer! Thanks to the amazing work of some amazing computer scientists and mathematicians, we are able to extract some of the semantic meaning from documents. However, we are still getting junk and are working on a few improvements to our system which will hopefully cut down on the amount of junk text getting through to this stage resulting in few junk subject-predicat-object triples. As always, if you want to see what we have been exploring, feel free to check out my <a href="https://github.com/alexandermichels/AIReading">AI Readings</a> GitHub Repo and for the latest in OpenIE go here: <a href="https://github.com/dair-iitd/OpenIE-standalone">https://github.com/dair-iitd/OpenIE-standalone</a></p>

</article>
