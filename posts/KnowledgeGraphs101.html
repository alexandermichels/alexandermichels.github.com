<!-- Blog Posts -->
<article>
  <header>
    <a href="https://alexandermichels.github.io/blog/KnowledgeGraphs101.html"><h2>Graph Knowledge 101</h2></a>
    <address>
      <a href="https://alexandermichels.github.io/about_me.html">Alexander Michels</a>, AlexanderMichels.GitHub.io
    </address>
    <time datetime="2018-07-26">Thursday, July 26<sup>th</sup> 2018</time>
  </header>

  <h2>Introduction</h2>

  <p>Knowledge Graphs are an increasingly popular data structure for representing relational information. Whether it is being used by <a href="https://www.ibm.com/blogs/research/2016/01/from-knowledge-graphs-to-cognitive-computing/">IBM's Watson</a> to <a href="https://www.youtube.com/watch?v=WFR3lOm_xhE">beat humans at Jeopardy</a> or <a href="https://searchengineland.com/library/google/google-knowledge-graph">Google's search engine to better understand the semantic meaning of queries and information</a>, they are springing up everywhere because they represent information in a manner that helps computers indentify the semantic meaning of the information they are processing, a key step towards cognitive computing.</p>

  <div align="center">
    <img alt="Common knowledge graphs" src="../images/KnowledgeGraphs101-1.png">
  </div>

  <h2>Types of Knowledge Graphs</h2>

  <h4>Open versus Closed Word Assumption</h4>

  <p>The Open World Assumption (OWA) means that if a subject-predicat-object statement of fact is not present in the knowledge graph, we do not know if it is true or false. We are <em>open</em> to a world of new facts. This is generally the best assumption to go with as knowledge graphs are generally incomplete.</p>

  <p>The Closed World Assumption (CWA) means that if a statement of fact is not present in the knowledge graph, we reject the fact as false. We are <em>closed</em> minded to a world of new facts. This approach can be justified after intensive training and when working with a large quantity of unreliable data.</p>

  <p>The Local-Closed World Assumption (LCWA) assumes that the Knowledge graph is locally complete, but globally incomplete. So when presented with a statement of fact (<em>i</em>, <em>j</em>, <em>k</em>), if we already have observed a subject-predicate pair (<em>i</em>, <em>j</em>, *) we will assume (<em>i</em>, <em>j</em>, <em>k</em>) is false. If we have not already observed a subject-predicate pair <em>i</em>,<em>j</em> than we put (<em>i</em>, <em>j</em>, <em>k</em>) in the unknown category.</p>

  <h4>Knowledge Base Construction</h4>

  <p>The information that informs the statements of fact in a knowledge graph can be gathered in a variety of ways, but the accuracy of the information greatly affects the usefulness of the final product. There are generally four approaches:</p>

  <ul>
    <li>curated approaches which feature experts manually constructing triples</li>
    <li>collaborative approaches which feature a group of volunteers manually constructing triples</li>
    <li>automated semistructured approaches which automatically extract information from semistructured sources using hand-crafted rules</li>
    <li>automated unstructured approaches which use Natural Language Processing and Machine Learning techniques</li>
  </ul>

  <h4>Schema-Based vs. Schema-Free</h4>

  <p>Schema-Based Knowledge Graphs are able to avoid a class of problems known as Entity Resolution (See "Challenges/Entity Resolution" for more info) because they have a pre-specified vocabulary of subjects and predicates with unique identifiers. The problem with this approach is the inability to learn facts that lie outside of the predefined vocabulary of relations.</p>

  <p>Schema-Free approaches use open information extraction to identify entities and relations and represent them using normalized by not disambiguated strings called "surface names." There are drawbacks to this approach in capturing semantic meaning.</p>

  <div align="center">
    <img alt="Common knowledge graphs" src="../images/KnowledgeGraphs101-2.png">
  </div>

  <h2>Properties of Knowledge Graphs</h2>

  <h4>Adjacency Tensors and Possible Worlds</h4>

  <p>Formally, a knowledge graph is an ordered pair <strong>G</strong>=(<strong>N</strong>,<strong>L</strong>) where <strong>N</strong> is a set of entity or concept nodes and <strong>L</strong> is a set of predicate links. Let <strong>E</strong> = {e<sub>1</sub>, e<sub>2</sub>,...,e<sub>N<sub>e</sub></sub>} be the set of entities in <strong>G</strong> and <strong>R</strong> = {r<sub>1</sub>, r<sub>2</sub>,...,r<sub>N<sub>r</sub></sub>} be the set of all relation types in <strong>G</strong>. We can model the truth of each statement of fact, (<em>i</em>, <em>j</em>, <em>k</em>) where <em>i</em>,<em>k</em> are elements of <strong>E</strong> and <em>j</em> is an element of <strong>R</strong>, by the binary random variable y<sub><em>ijk</em></sub> in {0,1}. This means that all possible triples in <strong>E</strong> x <strong>R</strong> x <strong>E</strong> can be expressed by a third-order tensor whose entries are set such that y<sub><em>ijk</em></sub> = 1 if (<em>i</em>, <em>j</em>, <em>k</em>) in <strong>G</strong> and 0 otherwise. Extending from the matrix vocabulary, we will refer to this construct as an <em>adjacency tensor</em>. Each possible realization of our adjacency tensor represents a possible world and we are interested in estimating the joint distribution over possible worlds to predict the probability of triples based on the state of the known triples in our knowledge graph.</p>

  <h4>Hierarchical Types and Transitivity</h4>

  <p>Due to the way Knowledge Graphs hold relational data, they generally establish hierarchies within their data because of type constraints which is extremely helpful when it comes to querying and Natural Language Processing. For example, ("Leonard Nimoy", "is a", "actor") and all "actors" are "human/people" thus ("Leonard Nimoy", "is a", "person").</p>

  <p>Along the same lines, the graph structure allows us to infer new facts through transitivity. ("Leonard Nimoy", "was born in", "Boston, MA") and ("Boston, MA", "is a city in", "United States"), thus ("Leonard Nimoy", "was born in", "United States").</p>

  <h4>Homophily</h4>

  <p>Besides mathematical/computer science theorems such as transitivity, there are "softer" patterns that can be exploited in graph representations of data. Among them is homophily or autocorrelation, or the tendency of individuals to associate and bond with similar others. This is also knows as link-based clustering or community detection. This is helpful for us because we can cluster entities based on links and from there infer links based on the what is known about similar entities. Social media does this when they realize that you and 30 other people all like the same 10 bands, so the social media platform recommends a band to you that the other 30 people also all like.</p>

  <div align="center">
    <img alt="Business knowledge graphs" src="../images/KnowledgeGraphs101-3.jpg">
  </div>

  <h4>Block Structures</h4>

  <p>Knowledge graphs tend to be able to be divided up into "blocks" which are sets of entities such that all of the members of the set have similar relationships to the members of another block. This is analogous to, a bipartite matching. An example of two blocks would be "Science Fiction writers" and "Science Fiction franchises". All entities in the "Science Fiction writers" block would have the same relation ("authored") to an entity in the "Science Fiction franchises" block.</p>

  <h4>Statement of Fact Correlation</h4>

  <p>The presence or absence of subject-predicate-object triples is correlated with, and thus predictive of, the presence or absence of certain other statements of facts. Mathematically, this means the random variables y<sub><em>ijk</em></sub> are correlated with one another. This can use what is known to predict what could be also be true. There are three main ways to model the correlations between the subject-predicate-object triples:</p>
  <ul>
    <li><strong>Latent Feature Models</strong> - Assume all y<sub><em>ijk</em></sub> are conditionally independent given the latent features associated with subject, object, and relational type and additional parameters</li>
    <li><strong>Graph Feature Models</strong> - Assume all y<sub><em>ijk</em></sub> are conditionally independent give observed graph features and additional parameters</li>
    <li><strong>Markov Random Fields</strong> - Assume all y<sub><em>ijk</em></sub> have local interactions</li>
  </ul>

  <p>The models using the Latent Feature Models and Graph Feature Models predict the existence of a statement of fact x<sub><em>ijk</em></sub> using a score function f(x<sub><em>ijk</em></sub>; &Theta;) representing the system's confidence in the existence of the fact given the parameters &Theta;. The model can be written as follows where <strong>Y</strong> is the third-order tensor and <em>D</em> is the set of observed triples:</p>

  <div align="center">
    <img alt="Probability model for the confidence of link prediction in a knowledge graph" src="../images/KGConfidenceOfPrediction.png">
  </div>

  <a style="float:right" href="https://github.com/alexandermichels/AIReading/blob/master/pdf/A_Review_of_Relational_Machine_Learning_for_Knowledge_Graphs.pdf">image credits</a>

  <p style="visibility:hidden">break<p>

  <p>Why am I writing about knowledge graphs? Well I found them because keyword-based classification wasn't quite working for our work and thanks to <a href="https://www.andrew.cmu.edu/user/abjorn/Site/Home.html">Dr. Adam Bjorndahl</a>'s kind words and reading recomendations on modal logic, we thought to use logic-based classification. Our thought classification ideally works as follows: if a document <strong>D</strong> contains a statement <strong>P</strong> which is contradictory to the set of possible-words, as described by our possible adjacency tensors, we can reject said document. Our RIPS team is hoping to use this concept in conjunction with computational fact-checking and knowledge graph concepts to implement a robust classification and credibility checking module. I'll be sure to talk more about our work soon, but for an overview of Knowledge Graphs and Computational Fact-Checking, see <a href="https://github.com/alexandermichels/AIReading/blob/master/pdf/Computational_Fact_Checking_from_Knowledge_Networks.pdf">Computational Fact Checking from Knowledge Networks</a> or <a href="https://github.com/alexandermichels/AIReading/blob/master/pdf/Towards_Computational_Fact-Checking.pdf">Towards Computational Fact-Checking</a>. I highly recommend <a href="https://github.com/alexandermichels/AIReading/blob/master/pdf/A_Review_of_Relational_Machine_Learning_for_Knowledge_Graphs.pdf">A Review of Relational Machine Learning for Knowledge Graphs</a> for the relational learning concepts. To see all the concepts and papers we are exploring, feel free to check out the "Knowledge Graphs and Computational Fact-Checking" and "Relational Learning" sections of my <a href="https://github.com/alexandermichels/AIReading">AIReading Github</a>.</p>

</article>
