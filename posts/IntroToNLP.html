
  <!-- Blog Posts -->
  <article>
    <header>
      <a href="https://alexandermichels.github.io/blog/IntroToNLP.html"><h2>Intro to NLP</h2></a>
      <address>
        <a href="https://alexandermichels.github.io/about_me.html">Alexander Michels</a>
      </address>
    </header>
    <p>As anyone who has taken an introductory programming course knows, language is terrible. Strings, how we represent multiple characters strung together, give every beginning computer scientist problems when writing code, especially when designing a basic, text-based interface for programs. This is because language in computer science, like many things computer science, follows the moto of "easier said than done." Yes, it is easy to say that "done", "Done", and "DONE" all represent the same concept, but to a computer they are all very different. In many languages for beginners, they are references to memory locations where the information is stored so asking if "done" and "Done" are the same isn't quite what you want do to, and even if it were, the computer will say they aren't because of the difference in capitalization despite the same semantic meaning. That may or may not be what you wanted, depending on the application, but because of the dense representations we use for computer systems, that is what we get.</p>

    <center><img alt="Diagram of NLP acting as an intermediary between information and applications" src="../images/IntroToNLP1-1.png" style="max-width:100%;" /></center>

    <p>So then we learn tips and tricks like equalsIgnoreCase in Java or just setting everything to lower for comparison and we learn how to search for strings within strings (substrings), but we are still just asking the computer to do simple arithmetic. Natural Language Processing (NLP) is where true understanding comes in. It's where we train a computer to "understand" the semantic meanings of the sequences of codes its doing arithmetic on and form associations with words. In the beginnings of the field, the researchers relied on a corpus (set of texts) to hard-code huge decision trees (think a giant flow chart for making sentences), but nowadays this is done using a variety of interesting methods which makes natural languages easier to process and then generally relies on probability, statistics, and machine learning. Lately my work at UCLA'a Institute for Pure and Applied Mathematics has had me doing a lot work with Natural Language Processing and I wanted to share an accessible synopsis of what I've learned in the hopes others will love it as much as I do!</p>

    <a href="https://en.wikipedia.org/wiki/Stop_words"><h2>Stop Words</h2></a>

    <p>Stop words, words that are filtered out before or after processing text, were first introduced to the field of information retrieval by <a href="https://en.wikipedia.org/wiki/Hans_Peter_Luhn">Hans Peter Luhn</a>. There is no definitive list of stop words, but generally stop words are common words in a language and words that serve a functional purpose rather than a lexical one. For example, "the", "an", and "a". A great example of stop word useage is search engines! Many search engines ignore short function words such as "the", "is", "at", "which", and "on." If you think about it, it makes a lot of sense. Let's say I want to search "Where to find the barber." The key words in that query are "Where/find" and "barber", both "where" and "find" convey the concept of searching for location and "barber" conveys that I am looking for someone offering haircuts. However if we treat all keywords the same, the words "to" and "the" will likely dominate the search results because of ubiquity of the words leaving me with useless results.</p>

    <center><img alt="NLP Word Bubble" src="../images/IntroToNLP1-2.jpg" style="max-width:100%;" /></center>

    <a href="https://en.wikipedia.org/wiki/Lemmatisation"><h2>Stemming and Lemmatisation</h2></a>
    <p>Stemming deals with removing the stem of words to leave just the root form of the word. For example, "ended" and "ending" would both be stemmed to be become "end" because the inflection of the word is removed. This essentially removes grammar from your language processing, but it also greatly reduces your dictionary (time and space efficient!) and allows you more effectively associate words. That example just stripped the suffix, but more complex versions of stemming use dictionary look-up to associate different conjugations and inflects of words. For example, let's say I am doing Natural Language Processing on a sports magazine. I would expect one of the most highly associated words with "track" would be "run," because people in the magazine are running track. However, without stemming "run" will be associated, but so will "ran", "runs", and "running" so other words might end up more highly associated with "track" even though "run", "runs", "ran", and "running" all carry the same meaning.</p>

    <p>Related is the idea of Lemmatisation which is a really interesting and open field of research that deals with grouping together words that have the same semantic or lexical meaning, like "better" and "good." Unlike stemming which operates on a single word without knowledge of the context, lemmatisation tries to take into the account the actual meaning of the word and map it a single "representative" for that concept, like how "good" would be a representive for the conept of "better" and "well." The "well" example is a difficult one however, how does a lemmatisation algorithm know to map "well" to "good" (i.e. "I am well") or leave it be (i.e. "I need water from the well")? Because lemmatisation uses context to make its decisions, and part of the information it takes into account is the part-of-speech of the word.</p>

    <center><img alt="Part-Of-Speech tree" src="../images/Syntax-Tree.png" style="max-width:100%;" /></center>

    <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging"><h2>Part-Of-Speech Tagging</h2></a>
    <p>Part-of-speech tagging is a very interesting technique used in corpus linguistics which attempts to tag each word in a document according to the part-of-speech it is. Put another way, it attempts to label the grammatical role each word is taking on. For example, give the sentence "I hate you" and using just the 9 basic parts of speech, we know that "I" and "you" are nouns and "hate" is a verb. However, there are many more categories and subcategories and many sentences are not nearly as clear-cut as that one. Consider the completely valid English sentence "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo." Please label the parts of speech for me. <a href="https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo">Decoded</a>, this means, "Buffalo (animal) from Buffalo (city in NY) that other buffalo (animal) from Buffalo (city) bully [themselves] bully buffalo (animal) from Buffalo (city)." This is obviously an extreme example, but it points to a common thread in POS tagging: words taking on many parts-of-speech. For example, consider "the dogs hound the cat" and "the hounds dog the cat." "Hound" and "dog" are both verbs and nouns and a computer can't easily distinguish what is going on here.</p>

    <p>That's a brief overview of the kinds of techniques and challenges faced in Natural Language Processing and Information Retrieval and I'll be following up over the next few weeks by talking about the specific NLP tasks I have been doing here at IPAM.</p>

  </article>
